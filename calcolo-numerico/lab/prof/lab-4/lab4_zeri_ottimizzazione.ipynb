{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab4_zeri_ottimizzazione.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRGdhVB8HA-F"
      },
      "source": [
        "# Lab 4: Zeri di funzione ed ottimizzazione"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRK_I6GrHM8k"
      },
      "source": [
        "## **Recap funzioni Python**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNi6mpDdGUFa"
      },
      "source": [
        "def upper_text(text):  \n",
        "    return text.upper()  \n",
        "\n",
        "upper_text2 = lambda text: text.upper()  \n",
        "  \n",
        "stringa = \"Hello world\" \n",
        "\n",
        "# chiamata alle funzioni\n",
        "\n",
        "print('upper_text: ', upper_text (stringa)) \n",
        "print('upper_text2: ', upper_text2 (stringa))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXInRUcxH08Y"
      },
      "source": [
        "def hello(func):  \n",
        "    # storing the function in a variable  \n",
        "    greeting = func(\"Greetings!\")  \n",
        "    print(greeting) \n",
        "\n",
        "hello(upper_text2)      \n",
        "\n",
        "def hello2(func, text = \"Hello world!\"):  \n",
        "    greeting = func(text)  \n",
        "    print(greeting)\n",
        "\n",
        "hello2(upper_text2, stringa)\n",
        "hello2(upper_text2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjhywP3rRrjX"
      },
      "source": [
        "# **Esercizio 1**: Calcolare lo zero di una funzione\n",
        "---\n",
        "\n",
        "Scrivere una function che implementi il metodo delle approssimazioni successive per il calcolo dello zero di una funzione f(x) per $x\\in \\mathbb{R}^{n}$ prendendo come input una funzione per l'aggiornamento:\n",
        "* $g(x)=x-f(x)e^{x/2}$\n",
        "* $g(x)=x-f(x)e^{-x/2}$\n",
        "* $g(x)=x-f(x)/f'(x)$\n",
        "\n",
        "Testare il risolutore per risolvere $f(x) = e^x − x^2 = 0$, la cui soluzione è $x^∗ = −0.7034674$. In particolare:\n",
        "* Disegnare il grafico della funzione $f$ nell’intervallo I = [−1,1] e verificare che $x^*$ sia lo zero di f in [-1, 1].\n",
        "* Calcolare lo zero della funzione utilizzando le funzioni precedentemente scritte.\n",
        "* Confrontare l'accuratezza delle soluzioni trovate e il numero di iterazioni effettuate dai solutori.\n",
        "* Modificare la function in modo da calcolare l'errore $||x_k -x^*||_2$ ad ogni iterazione k-esima e graficare  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24zHlKSPWiK4"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg4YHsmM4nHa"
      },
      "source": [
        "def succ_app(f, g, tolf, tolx, maxit, xTrue, x0=0):\n",
        "  i=0\n",
        "  err=np.zeros(maxit+1, dtype=np.float64)\n",
        "  err[0]=tolx+1\n",
        "  vecErrore=np.zeros(maxit+1, dtype=np.float64)\n",
        "  vecErrore[0] = np.abs(x0-xTrue)\n",
        "  x=x0\n",
        "\n",
        "  while (i<maxit and (err[i]>tolx or abs(f(x))>tolf) ): # scarto assoluto tra iterati\n",
        "    x_new= # TODO\n",
        "    err[i+1]= # TODO differenza iterate/relative change\n",
        "    vecErrore[i+1]= # TODO distanza soluzione reale\n",
        "    i=i+1\n",
        "    x=x_new\n",
        "  err=err[0:i]      \n",
        "  vecErrore = vecErrore[0:i]\n",
        "  return (x, i, err, vecErrore) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kp1fNIO5On2"
      },
      "source": [
        "f = # TODO\n",
        "df = # TODO\n",
        "\n",
        "g1 = # TODO\n",
        "g2 = # TODO\n",
        "g3 = # TODO\n",
        "\n",
        "xTrue = -0.7034674\n",
        "fTrue = f(xTrue)\n",
        "print(fTrue)\n",
        "\n",
        "tolx= 10**(-10)\n",
        "tolf = 10**(-6)\n",
        "maxit=100\n",
        "x0= 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvbQ0Nom5wSh"
      },
      "source": [
        "[sol_n, iter_n, err_n, vecErrore_g1] = # TODO\n",
        "print('Metodo approssimazioni successive g1 \\n x =',sol_n,'\\n iter_new=', iter_n, '\\n err_new=', err_n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnHm9mKd6fbU"
      },
      "source": [
        "[sol_n, iter_n, err_n, vecErrore_g2] = # TODO\n",
        "print('Metodo approssimazioni successive g2 \\n x =',sol_n,'\\n iter_new=', iter_n, '\\n err_new=', err_n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyRG-vLa8gA2"
      },
      "source": [
        "[sol_n, iter_n, err_n, vecErrore_g3] = # TODO\n",
        "print('Metodo approssimazioni successive g3 \\n x =',sol_n,'\\n iter_new=', iter_n, '\\n err_new=', err_n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JlOYgxseqOB"
      },
      "source": [
        "# GRAFICO Errore vs Iterazioni\n",
        "\n",
        "# g1\n",
        "plt.plot(vecErrore_g1, '.-', color='blue')\n",
        "# g2\n",
        "plt.plot(vecErrore_g2[:3], '.-', color='green')\n",
        "# g3\n",
        "plt.plot(vecErrore_g3, '.-', color='red')\n",
        "\n",
        "plt.legend( (\"g1\", \"g2\", \"g3\"))\n",
        "plt.xlabel('iter')\n",
        "plt.ylabel('errore')\n",
        "plt.title('Errore vs Iterazioni')\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3Bth6BuTq2m"
      },
      "source": [
        "# **Esercizio 2**: metodo del gradiente per l'ottimizzazione in $\\mathbb{R}^2$\n",
        "---\n",
        "Scrivere una funzione che implementi il metodo del gradiente rispettivamente con step size $\\alpha_k$ variabile, calcolato secondo la procedura di backtracking ad ogni iterazione k-esima.\n",
        "\n",
        "Testare la function per minimizzare $f(x)$ definita come: \n",
        "$$f(x)=10(x-1)^2+(y-2)^2$$\n",
        "\n",
        "In particolare:\n",
        "* plotta la superficie $f(x)$ con \n",
        "${\\tt plt.plot\\_surface()}$ e le curve di livello con ${\\tt plt.contour()}$.\n",
        "* plotta, al variare delle iterazioni, la funzione obiettivo, l'errore e la norma del gradiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_STNJcrWRzh"
      },
      "source": [
        "## **Superfici Python**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUMhSRNWTqXH"
      },
      "source": [
        "def f(x,y):\n",
        "    return (x-1)**2 - (y-2)**2\n",
        "\n",
        "\n",
        "x = np.linspace(-1.5,3.5,100)\n",
        "y = np.linspace(-1,5,100)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z=f(X,Y)\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "ax1 = plt.subplot(1, 2, 1, projection='3d')\n",
        "ax1.plot_surface(X, Y, Z, cmap='viridis')\n",
        "ax1.set_title('Surface plot')\n",
        "ax1.view_init(elev=20)\n",
        "\n",
        "ax2 = plt.subplot(1, 2, 2, projection='3d')\n",
        "ax2.plot_surface(X, Y, Z, cmap='viridis')\n",
        "ax2.set_title('Surface plot from a different view')\n",
        "ax2.view_init(elev=5)\n",
        "plt.show()\n",
        "\n",
        "#plt.figure(figsize=(8, 5))\n",
        "\n",
        "contours = plt.contour(X, Y, Z, levels=10)\n",
        "plt.title('Contour plot')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTDF5KUDZMOX"
      },
      "source": [
        "def next_step(x,b,grad): # backtracking procedure for the choice of the steplength\n",
        "  alpha=1.1\n",
        "  rho = 0.5\n",
        "  c1 = 0.25\n",
        "  p=-grad\n",
        "  j=0\n",
        "  jmax=10\n",
        "  while ((f(x[0]+alpha*p[0],x[1]+alpha*p[1],b) > f(x[0],x[1],b)+c1*alpha*grad.T@p) and j<jmax ):\n",
        "    alpha= rho*alpha\n",
        "    j+=1\n",
        "  if (j>jmax):\n",
        "    return -1\n",
        "  else:\n",
        "    #print('alpha=',alpha)\n",
        "    return alpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnfa_xyThGny"
      },
      "source": [
        "def minimize(x0,b,mode,step,MAXITERATION,ABSOLUTE_STOP): # funzione che implementa il metodo del gradiente\n",
        "  #declare x_k and gradient_k vectors\n",
        "  if mode=='plot_history':\n",
        "    x=np.zeros((2,MAXITERATION))\n",
        "\n",
        "  norm_grad_list=np.zeros((1,MAXITERATION))\n",
        "  function_eval_list=np.zeros((1,MAXITERATION))\n",
        "  error_list=np.zeros((1,MAXITERATION))\n",
        "  \n",
        "  #initialize first values\n",
        "  x_last = np.array([x0[0],x0[1]])\n",
        "\n",
        "  if mode=='plot_history':\n",
        "    x[:,0] = x_last\n",
        "  \n",
        "  k=0\n",
        "\n",
        "  function_eval_list[:,k]=f(x_last[0], x_last[1])\n",
        "  error_list[:,k]=np.linalg.norm(x_last-b)\n",
        "  norm_grad_list[:,k]=np.linalg.norm(grad_f(x_last))\n",
        "\n",
        "  while (np.linalg.norm(grad_f(x_last))>ABSOLUTE_STOP and k < MAXITERATION ):\n",
        "    k=k+1\n",
        "    grad = # TODO direction is given by gradient of the last iteration\n",
        "\n",
        "    # backtracking step\n",
        "    step = # TODO\n",
        "    # Fixed step\n",
        "    #step = 0.1\n",
        "    \n",
        "    if(step==-1):\n",
        "      print('non convergente')\n",
        "      return (iteration) # no convergence\n",
        "\n",
        "    x_last= # TODO gradient update\n",
        "\n",
        "    if mode=='plot_history':\n",
        "      x[:,k] = x_last\n",
        "\n",
        "    function_eval_list[:,k]=f(x_last[0], x_last[1])\n",
        "    error_list[:,k]=np.linalg.norm(x_last-b)\n",
        "    norm_grad_list[:,k]=np.linalg.norm(grad_f(x_last))\n",
        "\n",
        "  function_eval_list = function_eval_list[:,:k+1]\n",
        "  error_list = error_list[:,:k+1]\n",
        "  norm_grad_list = norm_grad_list[:,:k+1]\n",
        "  \n",
        "  print('iterations=',k)\n",
        "  print('last guess: x=(%f,%f)'%(x[0,k],x[1,k]))\n",
        " \n",
        "  #plots\n",
        "  if mode=='plot_history':\n",
        "    v_x0 = np.linspace(-5,5,500)\n",
        "    v_x1 = np.linspace(-5,5,500)\n",
        "    x0v,x1v = np.meshgrid(v_x0,v_x1)\n",
        "    z = f(x0v,x1v,b)\n",
        "    \n",
        "    plt.figure()\n",
        "    ax = plt.axes(projection='3d')\n",
        "    ax.plot_surface(v_x0, v_x1, z,cmap='viridis')\n",
        "    ax.set_title('Surface plot')\n",
        "    plt.show()\n",
        "\n",
        "    # plt.figure(figsize=(8, 5))\n",
        "    contours = plt.contour(x0v, x1v, z, levels=30)\n",
        "    plt.plot(x[0,0:k],x[1,0:k],'*')\n",
        "    #plt.axis([-5,5,-5,5])\n",
        "    plt.axis ('equal')\n",
        "    plt.show()\n",
        "  return (x_last,norm_grad_list, function_eval_list, error_list, k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXqn3jrOlSkg"
      },
      "source": [
        "b=np.array([1,2])\n",
        "\n",
        "def f(x1,x2,b=b):\n",
        "  res = # TODO\n",
        "  return res\n",
        "\n",
        "def grad_f(x,b=b):\n",
        "  return # TODO\n",
        "\n",
        "#step=0.1\n",
        "MAXITERATIONS=1000\n",
        "ABSOLUTE_STOP=1.e-5\n",
        "mode='plot_history'\n",
        "x0 = np.array((3,-5))\n",
        "\n",
        "(x_last,norm_grad_list, function_eval_list, error_list, k)= # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBk1ABSclbTt"
      },
      "source": [
        "plt.plot(norm_grad_list.T, 'o-')\n",
        "plt.xlabel('iter')\n",
        "plt.ylabel('Norma Gradiente')\n",
        "plt.title('Iterazioni vs Norma Gradiente')\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ab2Xh_YmyHk"
      },
      "source": [
        "plt.plot(error_list.T, 'o-')\n",
        "plt.xlabel('iter')\n",
        "plt.ylabel('Errore')\n",
        "plt.title('Errore vs Iterazioni')\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxx4-edpn03W"
      },
      "source": [
        "plt.plot(function_eval_list.T, 'o-')\n",
        "plt.xlabel('iter')\n",
        "plt.ylabel('Funzione Obiettivo')\n",
        "plt.title('Iterazioni vs Funzione Obiettivo')\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87ROooEbpzlH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}